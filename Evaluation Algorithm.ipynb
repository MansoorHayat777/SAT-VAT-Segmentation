{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "# Function to binarize an array based on a given threshold\n",
    "def binarize(array, threshold=0.5):\n",
    "    return (array > threshold).astype(int)\n",
    "\n",
    "# Function to calculate the values of the confusion matrix\n",
    "def calculate_confusion_matrix_values(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    return tp, tn, fp, fn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Paths to segmentation files and ground truth files\n",
    "path_to_segmentations = \"PATH TO PREDICTED SEGMENTATION FILES\"\n",
    "path_to_ground_truth = \"PATH TO GROUND TRUTH SEGMENTATION FILES\"\n",
    "\n",
    "# List the segmentation and ground truth files\n",
    "segmentation_files = os.listdir(path_to_segmentations)\n",
    "ground_truth_files = os.listdir(path_to_ground_truth)\n",
    "\n",
    "# Initialize a dictionary to store various metrics for each file\n",
    "metrics = {\n",
    "    'file': [], 'dice': [], 'jaccard': [], 'precision': [],\n",
    "    'recall': [], 'accuracy': [], 'TP': [], 'TN': [], 'FP': [], 'FN': []\n",
    "}\n",
    "\n",
    "# Loop through the files and calculate the metrics\n",
    "for index in range(len(segmentation_files)):\n",
    "    # Construct full file paths\n",
    "    segmentation_file_name = path_to_segmentations + segmentation_files[index]\n",
    "    ground_truth_file_name = path_to_ground_truth + ground_truth_files[index]\n",
    "\n",
    "    # Read and process (convert to float) the images\n",
    "    segmentation_image = img_as_float(io.imread(segmentation_file_name, as_gray=True))\n",
    "    ground_truth_mask = img_as_float(io.imread(ground_truth_file_name, as_gray=True))\n",
    "\n",
    "    # Binarize the images\n",
    "    segmentation_image = binarize(segmentation_image)\n",
    "    ground_truth_mask = binarize(ground_truth_mask)\n",
    "\n",
    "    # Flatten the images to 1D for metric calculations\n",
    "    segmentation_flat = segmentation_image.flatten()\n",
    "    ground_truth_flat = ground_truth_mask.flatten()\n",
    "\n",
    "    # Compute different metrics\n",
    "    dice = f1_score(ground_truth_flat, segmentation_flat)\n",
    "    jaccard = jaccard_score(ground_truth_flat, segmentation_flat)\n",
    "    precision = precision_score(ground_truth_flat, segmentation_flat)\n",
    "    recall = recall_score(ground_truth_flat, segmentation_flat)\n",
    "    accuracy = accuracy_score(ground_truth_flat, segmentation_flat)\n",
    "    tp, tn, fp, fn = calculate_confusion_matrix_values(ground_truth_flat, segmentation_flat)\n",
    "\n",
    "    # Append metrics to the dictionary\n",
    "    metrics['file'].append(segmentation_files[index])\n",
    "    metrics['dice'].append(dice)\n",
    "    metrics['jaccard'].append(jaccard)\n",
    "    metrics['precision'].append(precision)\n",
    "    metrics['recall'].append(recall)\n",
    "    metrics['accuracy'].append(accuracy)\n",
    "    metrics['TP'].append(tp)\n",
    "    metrics['TN'].append(tn)\n",
    "    metrics['FP'].append(fp)\n",
    "    metrics['FN'].append(fn)\n",
    "\n",
    "# Create DataFrame from the metrics and calculate the average\n",
    "metrics_dataframe = pd.DataFrame(metrics)\n",
    "average_metrics = metrics_dataframe[['dice', 'jaccard', 'precision', 'recall', 'accuracy', 'TP', 'TN', 'FP', 'FN']].mean(numeric_only=True)\n",
    "average_row = ['Average'] + average_metrics.tolist()\n",
    "\n",
    "# Append average row to DataFrame\n",
    "metrics_dataframe.loc[len(metrics_dataframe)] = average_row\n",
    "metrics_dataframe = metrics_dataframe.round(4)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "metrics_dataframe.to_excel(\"/metrics.xlsx\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
